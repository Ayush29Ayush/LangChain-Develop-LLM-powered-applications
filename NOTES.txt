Refer => https://www.udemy.com/course/langchain/

#* LangChain- Develop LLM powered applications with LangChain

#? Section - 1 : Introduction

#! Course Objectives

*  Develop LLM powered applications with LangChain

1) We can categorize LLM applications into 2 types:
    * Agents
    * Retrieval Augmentation Generation (RAG)

2) Be effiecient with:
    * LangChain Ecosystem: LangSmith (for tracing), LangChain Hub (for models), LangGraph (for workflow engineering), etc.
    * Prompt Engineering
    * Production

3) Also know:
    * Prompt engineering best practices
    * Prompt engineering techniques
    * History of prompting
    * Production ready topics like testing, logging, monitoring, alerting, security, etc.

4) Target Audience
    * Software Engineers
    * Data Scientists
    * Data Engineers
    * Data Analysts

------------------------------------------------------------------------------------------------------------------------------

#? Section - 2 : Gist of LangChain

#! What is LangChain - V1 

LangChain is a comprehensive framework designed to simplify the development of applications powered by large language models (LLMs). It provides standardized interfaces and tools to build, deploy, and maintain LLM-powered applications effectively.

The diagram (LangChain Image) illustrates LangChain's architecture, where:

- LangChain serves as the central framework, providing the foundational infrastructure
- LangGraph enables developers to build sophisticated, stateful applications with LLMs
- LangSmith handles monitoring and evaluation of deployed applications

Key features include:

1. **Development Simplification**  - Standardized interfaces for LLM integration
  - Extensive third-party integrations
  - Comprehensive development toolkit

2. **Core Capabilities**  - Integration with hundreds of LLM providers
  - Support for embedding models and vector stores
  - Built-in tools for common AI tasks

3. **Practical Implementation**

Simple setup example:
```python
pip install langchain-groq

from langchain_groq import ChatGroq
model = ChatGroq(model="llama3-8b-8192")
```

The framework supports various use cases including chatbots, question-answering systems, and document processing applications. Its modular design allows developers to build both simple prototypes and complex enterprise-level applications while maintaining scalability and reliability.

#! What is LangChain - V2

1) Definition and Purpose

    * An open-source framework that simplifies building LLM-powered applications
    * Provides tools and abstractions for creating complex LLM applications
    * Enables developers to build LLM applications without requiring deep ML knowledge
    * Most popular framework for developing LLM-powered applications

2) Core Functionality

    * Abstracts the complexity of data source integrations and prompt refining
    * Enables combining LLMs with personal data sources (PDFs, emails, databases)
    * Facilitates dynamic prompt construction based on user input
    * Manages message history between users and AI
    * Provides integration capabilities with external tools (Google Search, APIs)

3) Key Modules and Features

    Chat Models Module
        * Abstracts interaction with LLMs
        * Allows easy switching between different models
        * Provides standardized interface across all LLM vendors
        * Prevents vendor lock-in
    Prompt Management System
        * Handles prompt templates and optimization
        * Supports dynamic injection of user input
        * Enables prompt serialization
        * Promotes application composability
    Document Loaders
        * Facilitates loading various data sources
        * Unifies different data formats
        * Provides consistent interface for document handling
        * Streamlines data preprocessing for LLMs
    Agent Ecosystem
        * Supports building agentic applications
        * Enables LLM reasoning capabilities
        * Integrates tool invocation (search, database queries, email)
        * Includes abstractions for agents, executors, and link graphs

4) Community and Development

    * Available on GitHub with open-source code
    * Active community of contributors
    * Thorough documentation and resources
    * Regular updates and improvements
    * Production-ready features including monitoring and tracing

5) Development Benefits

    * Reduces development complexity
    * Provides modular architecture
    * Offers extensive customization options
    * Supports rapid prototyping
    * Facilitates production deployment

#! Project Setup

1. Create directory named 1_ice_breaker
2. python3 -m venv venv
3. source venv/bin/activate
4. pip install -r requirements.txt
5. Create .env file
6. Create ice_breaker_v1.py file

Packages installed:
    * langchain - used to build LLM-powered applications
    * langchain-openai - integration with OpenAI LLMs
    * langchain-groq - integration with Groq LLMs
    * langchain-community - community extensions for LangChain like text splitters, etc.
    * langchainhub - integration with LangChainHub for prompt templates
    * python-decouple - used to manage environment variables

#! Your First LangChain application - Chaining a simple prompt

1) Gist of LangChain (Simple Application)
    * Prompt template - they are used to create dynamic prompts for LLMs
    * Chat model - wrappers used to interact with LLMs
    * Chains - used to combine multile components together like connect prompt templates and chat models and create one single coherent application
    * Working summary example

#TODO => Refer => https://python.langchain.com/docs/concepts/

```
from decouple import config
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
import openai

information = """
    Elon Reeve Musk is a businessman and U.S. Special Government employee, best known for his key roles in Tesla, Inc., SpaceX, and his ownership of Twitter. Musk is the wealthiest individual in the world; as of January 2025, Forbes estimates his net worth to be US$426 billion. Musk's actions and expressed views have further solidified his status as a public figure.A member of the wealthy South African Musk family, Musk was born in Pretoria before immigrating to Canada, acquiring its citizenship. He moved to California in 1995 to attend Stanford University, and with his brother Kimbal co-founded the software company Zip2, that was later acquired by Compaq in 1999. That same year, Musk co-founded X.com, a direct bank, that later formed PayPal. In 2002, Musk acquired U.S. citizenship, and eBay acquired PayPal. Using the money he made from the sale, Musk founded SpaceX, a spaceflight services company, in 2002. In 2004, Musk was an early investor in electric vehicle manufacturer Tesla and became its chairman and later CEO. In 2018, the U.S. Securities and Exchange Commission (SEC) sued Musk, alleging he falsely announced that he had secured funding for a private takeover of Tesla, stepped down as chairman, and paid a fine. In 2022, he acquired Twitter, and rebranded the service as X the following year. In January 2025, Musk was appointed director of the Department of Government Efficiency as a special government employee. 
"""

if __name__ == "__main__":
    print("Hello LangChain!")

    summary_template = """
        Given {information} about a person, create:

        1. A 2-3 sentence summary including:
            - Current role, key achievements, and current status.
        2. Two verified facts with dates/details.

        Format with markdown headers and bullet points:
        ## Summary
        ## Interesting Facts
        1. [Fact 1]
        2. [Fact 2]

        Ensure professional tone and factual accuracy. 
    """


    summary_prompt_template = PromptTemplate(
        template=summary_template, input_variables=["information"]
    )

    # llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, api_key=config("OPENAI_API_KEY"))
    llm = ChatGroq(model="llama3-8b-8192", temperature=0, api_key=config("GROQ_API_KEY"))

    chain = summary_prompt_template | llm

    #! For OpenAI
    # try:
    #     res = chain.invoke(input={"information": information})
    #     print(res)
    # except openai.RateLimitError:
    #     print("Rate limit exceeded. Please wait and try again later.")
        
    #! For Groq
    try:
        res = chain.invoke(input={"information": information})
        print(res.content)
    except Exception as e:  # Catching general exception
        print(f"Error: {str(e)}")
        if "rate limit" in str(e).lower():
            print("Rate limit exceeded. Please wait and try again later.")
```

For Groq Integration => https://python.langchain.com/docs/integrations/chat/groq/

#! Using Open Source Models With LangChain (Ollama, Llama3, Mistral)

1. Download Ollama in linux => curl -fsSL https://ollama.com/install.sh | sh
2. Open terminal and type => ollama run llama3
3. pip install langchain-ollama
4. Create ice_breaker_v2.py and write the latest code

For Ollama Integration => https://python.langchain.com/docs/integrations/chat/ollama/

* LangChain also offers output parsers which can be used to parse the output of LLMs to extract specific information. This can be used rather than printing the output directly to the console.

```
from decouple import config
from langchain_core.prompts import PromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

information = """
    Pizza without cheese
"""

if __name__ == "__main__":
    print("Hello LangChain!" + "\n")

    summary_template = """
        Write a very short song on the topic : {information}
    """


    summary_prompt_template = PromptTemplate(
        template=summary_template, input_variables=["information"]
    )

    llm = ChatOllama(model="llama3", temperature=0)

    # chain = summary_prompt_template | llm 
    chain = summary_prompt_template | llm | StrOutputParser()
    
    #! For Ollama
    try:
        res = chain.invoke(input={"information": information})
        # print(res.content)
        print(res)
    except Exception as e:
        print(f"Error: {str(e)}")
```

------------------------------------------------------------------------------------------------------------------------------

#? Section - 3 : Ice Breaker Real World Generative AI Agent Application

#! LangChain Topics Covered
    * Chains
    * Agents
    * Custom Agents
    * Tools, Toolkit
    * Output Parsers

#! Integrating LinkedIn Data Processing - Part 1 - Scraping

Topics covered:
    * Web Scraping - Scrapin.io API
    * Data Cleanup
    * Testing 

1. Use Scrapin.io API to scrape LinkedIn profiles
2. Create a file named linkedin.py 
3. For testing purposes, create a github gist and store the scraped json data in the gist to avoid API rate limits
4. Create ice_breaker_v3.py and write the latest code

** linkedin.py **
```
import os
import requests
from decouple import config

def scrape_linkedin_profile(linkedin_profile_url: str, mock: bool = True):
    """scrape information from LinkedIn profiles,
    Manually scrape the information from the LinkedIn profile"""

    if mock:
        linkedin_profile_url = "https://gist.githubusercontent.com/Ayush29Ayush/c42be02c67249609306805ccf8434e2a/raw/de55b3daad3bd36130f1250eb5dc507f8f444edf/ayush-senapati-linkedin-scraping.json"
        response = requests.get(linkedin_profile_url,timeout=10)
    else:
        api_endpoint = "https://api.scrapin.io/enrichment/profile"
        params = {
            "apikey": config("SCRAPIN_API_KEY"),
            "linkedInUrl": linkedin_profile_url,
        }
        response = requests.get(api_endpoint,params=params,timeout=10)

    data = response.json().get("person")

    filtered_data = {}

    for key, value in data.items():
        if value not in ([], "", "", None) and key != "certifications":
            filtered_data[key] = value

    # print("Filtered Data =>", filtered_data)
    return filtered_data

if __name__ == "__main__":
    print(scrape_linkedin_profile(linkedin_profile_url="https://www.linkedin.com/in/ayush-senapati-a531b8145/",mock=True))
```

** ice_breaker_v3.py **
```
from decouple import config
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser

from third_parties.linkedin import scrape_linkedin_profile

if __name__ == "__main__":
    print("Hello LangChain!")

    summary_template = """
        Based on the LinkedIn information provided about the individual: {information}, please generate the following:

        1. A concise professional summary.
        2. A detailed overview of their current job role, 
        3. A detailed overview of their key achievements and professional status.
        4. Two intriguing and unique facts about the individual.

        Thank you!
    """

    summary_prompt_template = PromptTemplate(
        template=summary_template, input_variables=["information"]
    )

    llm = ChatGroq(model="llama3-8b-8192", temperature=0, api_key=config("GROQ_API_KEY"))

    chain = summary_prompt_template | llm | StrOutputParser()
    
    linkedin_data = scrape_linkedin_profile(linkedin_profile_url="https://www.linkedin.com/in/ayush-senapati-a531b8145/", mock=True)

    try:
        res = chain.invoke(input={"information": linkedin_data})
        print(res)
    except Exception as e:
        print(f"Error: {str(e)}")
```

#! Linkedin Data Processing - Part 2 - Agents Theory

* Agents - They are used to create applications that can perform tasks automatically based on user input
* Think of agents as bots, they will perform actions on your behalf and it is able to interact with the llms.
* Agents break down complex tasks into smaller subtasks and then combine the results to get the final output.
#TODO => Refer image => 04. LangChain Agents.png

#! Linkedin Data Processing- Part 3: Tools, Agent Executor, create_react_agent

* Tools - Tools are used to perform actions on your behalf and it is able to interact with the llms.
* Agent Executor - Agent Executors are used to execute agents and it is able to interact with the llms.
* create_react_agent - It is a function that creates a React agent.

1. Create a agents directory and create a file named linkedin_lookup_agent.py
** linkedin_lookup_agent.py **
```
from decouple import config
from langchain_groq import ChatGroq
from langchain.prompts.prompt import PromptTemplate
from langchain_core.tools import Tool
from langchain.agents import (
    create_react_agent,
    AgentExecutor,
)
from langchain import hub
```

#! Linkedin Data Processing- Part 4: Custom Search Agent Implementation

1. Create a tools directory and create a file named tools.py
2. We will be using Tavily API to search for Linkedin Profiles
3. Complete the code in linkedin_lookup_agent.py and use the get_profile_url_tavily function from tools.py

** tools.py **
```
from langchain_community.tools.tavily_search import TavilySearchResults
# from langchain.utilities.tavily_search import TavilySearchAPIWrapper
from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from decouple import config

def get_profile_url_tavily(name: str):
    """
    Searches for a person's LinkedIn or Twitter profile URL using Tavily Search.
    
    Args:
        name (str): Full name of the person to search for
        
    Returns:
        str: URL of the matching social media profile
    """
        
    # Retrieve Tavily API key from environment configuration
    tavily_api_key = config("TAVILY_API_KEY")
    
    # Initialize API wrapper with authentication credentials
    api_wrapper = TavilySearchAPIWrapper(tavily_api_key=tavily_api_key)
    
    # Create search client with authenticated wrapper
    search = TavilySearchResults(api_wrapper=api_wrapper)
    
    # Execute search with formatted query string
    res = search.run(f"{name}")
    
    # Extract and return the first matching URL
    return res[0]["url"]
```

** linkedin_lookup_agent.py **
```
import sys
import os

# Add parent directory to path for module imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from decouple import config  # For secure environment variable handling
from langchain_groq import ChatGroq  # Groq LLM integration
from langchain.prompts.prompt import PromptTemplate  # Template system for prompts
from langchain_core.tools import Tool  # Base class for custom tools
from langchain.agents import (  # Agent creation utilities
    create_react_agent,  # Creates a reactive agent
    AgentExecutor,  # Executes agent actions
)
from tools.tools import get_profile_url_tavily  # Custom tool for LinkedIn searching
from langchain import hub  # Hub for loading predefined prompts


def lookup(name: str) -> str:
    """
    Looks up a person's LinkedIn profile URL using a combination of LLM inference
    and web searching.

    Args:
        name (str): Full name of the person to look up

    Returns:
        str: LinkedIn profile URL
    """
    # Initialize Groq LLM model with temperature=0 for deterministic responses
    llm = ChatGroq(
        model="llama3-8b-8192", temperature=0, api_key=config("GROQ_API_KEY")
    )

    # Define prompt template for LinkedIn URL extraction
    # This template expects a single variable {name_of_person}
    template = """given the full name {name_of_person} I want you to get it me a link
                 to their Linkedin profile page. Your answer should contain only a URL"""

    # Create template instance with variable mapping
    prompt_template = PromptTemplate(
        template=template, input_variables=["name_of_person"]
    )

    # Define custom tool for LinkedIn profile searching
    tools_for_agent = [
        Tool(
            name="Crawl Google for linkedin profile page",
            func=get_profile_url_tavily,  # Pass function reference without parentheses
            description="useful for when you need get the Linkedin Page URL",
        )
    ]

    # Load predefined react prompt template
    react_prompt = hub.pull("hwchase17/react")

    # Create agent with LLM, tools, and prompt
    agent = create_react_agent(llm=llm, tools=tools_for_agent, prompt=react_prompt)

    # Create executor instance for running agent actions
    agent_executor = AgentExecutor(
        agent=agent, tools=tools_for_agent, verbose=True  # Enable detailed logging
    )

    # Execute the agent with formatted prompt
    result = agent_executor.invoke(
        input={"input": prompt_template.format_prompt(name_of_person=name)}
    )

    # Print result for debugging
    # print("Linkedin Lookup Agent Result =>", result)

    # Extract and return the LinkedIn URL
    linked_profile_url = result["output"]
    return linked_profile_url


if __name__ == "__main__":
    # Example usage with test case
    print(lookup(name="Ayush Senapati Cozentus VIT"))
```

* Read the comments in linkedin_lookup_agent.py file to understand the code implementation

#! Linkedin Data Processing- Part 5: Custom Search Agent Testin

1. Create ice_breaker_v4.py file
2. Integrate the linkedin_lookup_agent.py file which returns the user's Linkedin Profile URL
3. Use that url output from linkedin_lookup_agent.py and pass it to scrape_linkedin_profile function

#TODO => Summary:
1) We created a search agent which has access to a search tool
2) We used the llm in order to reason and decide which tool to use
3) The agent keeps on quering the search tool until it gets the desired output
4) We used the output of the search agent to pass it to the scrape_linkedin_profile function
5) We used the output of scrape_linkedin_profile to print the summary using the chain method which we wrote

#TODO => Overall order of Execution:
1) Create a tools directory and create a file named tools.py
2) Create a agents directory and create a file named linkedin_lookup_agent.py
3) Create a ice_breaker_v4.py file

#! Output Parsers- Getting Ready to work with a Frontend

Topics Covered byfar (in order):
    * Chat Models
    * Chains
    * Agents
    * Tools

* Right now the output provided by the agent is a string.
* We need to parse the output in json format to extract specific information from it.
* We will use "Output Parsers" to do that

1) We will use Pydantic to create a model having 2 fields i.e "summary" and "facts"
2) Then use the PydanticOutputParser to parse the output of the agent from a string to a Pydantic model's json or dictionary format and then return the parsed output.

* Create a file named output_parsers.py and write the parsing code from string to dictionary.
* Create a file named ice_breaker_v5.py file

** output_parsers.py **
```
from typing import List, Dict, Any
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# Define a data model to structure information about a person
class Summary(BaseModel):
    # Main summary text about the person
    summary: str = Field(description="summary")
    
    # Collection of interesting facts about the person
    facts: List[str] = Field(description="interesting facts about them")
    
    # Convert the model instance to a dictionary format
    def to_dict(self) -> Dict[str, Any]:
        return {"summary": self.summary, "facts": self.facts}

# Create a parser that will convert LLM output into our structured Summary model
summary_parser = PydanticOutputParser(pydantic_object=Summary)
```

** ice_breaker_v5.py **
```
from decouple import config
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
from third_parties.linkedin import scrape_linkedin_profile
from agents.linkedin_lookup_agent import lookup as linkedin_lookup_agent
from output_parsers import summary_parser

def ice_break_with(name: str) -> str:
    """
    Creates an engaging icebreaker summary from LinkedIn profile data using AI.
    
    Args:
        name (str): Full name of the person to create icebreaker for
        
    Returns:
        str: Structured summary containing professional information and interesting facts
    """
    
    # First, get the LinkedIn profile URL using our lookup agent
    linkedin_username_link = linkedin_lookup_agent(name=name)
    print("Linkedin Profile URL =>", linkedin_username_link)
    
    # Scrape the LinkedIn profile data (using mock data for testing)
    linkedin_data = scrape_linkedin_profile(
        linkedin_profile_url=linkedin_username_link, 
        mock=True
    )
    
    # Define template for generating professional summary
    summary_template = """
    Based on the LinkedIn information provided about the individual: {information}, please generate the following:
    1. A concise professional summary.
    2. A detailed overview of their current job role,
    3. A detailed overview of their key achievements and professional status.
    4. Two intriguing and unique facts about the individual.
    Thank you!
    \n {format_instructions}
    """
    
    # Create prompt template with format instructions for structured output
    summary_prompt_template = PromptTemplate(
        template=summary_template, 
        input_variables=["information"], 
        partial_variables={"format_instructions": summary_parser.get_format_instructions()}
    )
    
    # Initialize Groq LLM with deterministic responses
    llm = ChatGroq(
        model="llama3-8b-8192", 
        temperature=0, 
        api_key=config("GROQ_API_KEY")
    )
    
    # Create processing chain: template -> LLM -> string parser -> summary parser
    chain = summary_prompt_template | llm | StrOutputParser() | summary_parser
    
    try:
        # Execute the chain with LinkedIn data
        res = chain.invoke(input={"information": linkedin_data})
        print("res =>", res)
        print("Type of res =>", type(res), "\n")
        # Convert structured output to dictionary/json format
        res_dict = res.to_dict()
        print("res_dict =>", res_dict)
        print("Type of res_dict =>", type(res_dict))
    except Exception as e:
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    print("Hello LangChain!" + "\n")
    print("--" * 50)
    ice_break_with(name="Ayush Senapati Cozentus VIT")
    print("--" * 50)
    print("\n" + "Bye LangChain!" + "\n")
```

** The `summary_parser` serves several important purposes in our system:

1. **Structured Data Management**  - Ensures consistent output format
  - Validates that all required fields are present
  - Provides type safety with `summary: str` and `facts: List[str]`


2. **Error Prevention**  - Catches missing or malformed data early
  - Validates data types automatically
  - Prevents runtime errors from incorrect data structure


3. **Code Organization**  - Separates data structure definition from processing logic
  - Makes the code more maintainable and reusable
  - Provides a clear contract for expected data format


4. **Type Safety**  - Enforces proper typing throughout the system
  - Makes IDE autocompletion work correctly
  - Helps catch errors during development rather than runtime

The parser acts as a bridge between the free-form LLM output and our structured data needs, ensuring that we always have properly formatted, validated data to work with.

#! FullsStack App- Building our LLM powered by LangChain FullStack Application

Now we will:
    * Build server side 
    * Build client side
    * Have a end to end working application

* Create a file named ice_breaker_v6.py
* Create a file named app_flask.py
* create a html file named index.html

#! Tracing application with LangSmith

LangSmith is a tool for tracing LLM applications. It enables you to monitor and evaluate the performance and behavior of your LLM-powered applications. It provides a rich set of features, including:

1. **Integration**: LangSmith can be seamlessly integrated into your application's codebase, making it easy to trace and monitor LLM interactions.
2. **Real-time monitoring**: LangSmith provides real-time monitoring capabilities, allowing you to monitor and analyze the performance of your LLM-powered applications.
3. **Performance analysis**: LangSmith can help you identify performance bottlenecks and optimize your LLM-powered applications for better performance.

Use LangSmith if you want to trace:
    * LLM calls
    * Tool usage
    * LLM model latency
    * Token count
    * Cost analysis

Just add these to your .env:
```
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY=YOUR_VALUE
LANGSMITH_PROJECT="Ice Breaker"
```

and use them in your ice_breaker_v7 code file:
```
os.environ["LANGSMITH_TRACING"] = config("LANGSMITH_TRACING")
os.environ["LANGSMITH_ENDPOINT"] = config("LANGSMITH_ENDPOINT")
os.environ["LANGSMITH_API_KEY"] = config("LANGSMITH_API_KEY")
os.environ["LANGSMITH_PROJECT"] = config("LANGSMITH_PROJECT")
```

* refer LangSmith Docs => https://smith.langchain.com

------------------------------------------------------------------------------------------------------------------------------